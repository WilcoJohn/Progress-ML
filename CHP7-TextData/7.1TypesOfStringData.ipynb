{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b95524e-9ff7-49b3-951d-2eafb4a602bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import cm\n",
    "import pandas\n",
    "import mglearn\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import sklearn.ensemble              # import seperatley otherwise sub module won't be imported\n",
    "import sklearn.neural_network        # import seperatley otherwise sub module won't be imported\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.feature_selection\n",
    "\n",
    "import graphviz\n",
    "import mpl_toolkits.mplot3d as plt3dd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e322f-029a-4f4a-a74e-2a8b0a8288ee",
   "metadata": {},
   "source": [
    "# Data represented as strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c468793-1304-49c3-a805-a8755df60804",
   "metadata": {},
   "source": [
    "There are four kinds of string data you might see:\n",
    "\n",
    "- Categorical data\n",
    "- Free strings that can be semantically mapped to categories\n",
    "- Structured string data\n",
    "- Text data\n",
    "\n",
    "Categorical data is data that comes from a fixed list. Say you collect data via a survey where you ask people their favorite color, with a drop-down menu that allows them to select from “red,” “green,” “blue,” “yellow,” “black,” “white,” “purple,” and “pink.” This will result in a dataset with exactly eight different possible values, which clearly encode a categorical variable. You can check whether this is the case for your data by eyeballing it (if you see very many different strings it is unlikely that this is a categorical variable) and confirm it by computing the unique values over the dataset, and possibly a histogram over how often each appears. You also might want to check whether each variable actually corresponds to a category that makes sense for your application. Maybe halfway through the existence of your survey, someone found that “black” was misspelled as “blak” and subsequently fixed the survey. As a result, your dataset contains both “blak” and “black,” which correspond to the same semantic meaning and should be consolidated\n",
    "\n",
    "\n",
    "Now imagine instead of providing a drop-down menu, you provide a text field for the users to provide their own favorite colors. Many people might respond with a color name like “black” or “blue.” Others might make typographical errors, use different spellings like “gray” and “grey,” or use more evocative and specific names like “midnight blue.” You will also have some very strange entries. Some good examples come from the xkcd Color Survey, where people had to name colors and came up with names like “velociraptor cloaka” and “my dentist’s office orange. I still remember his dandruff slowly wafting into my gaping yaw,” which are hard to map to colors automatically (or at all). The responses you can obtain from a text field belong to the second category in the list, free strings that can be semantically mapped to categories. It will probably be best to encode this data as a categorical variable, where you can select the categories either by using the most common entries, or by defining categories that will capture responses in a way that makes sense for your application. You might then have some categories for standard colors, maybe a category “multicolored” for people that gave answers like “green and red stripes,” and an “other” category for things that cannot be encoded otherwise. This kind of preprocessing of strings can take a lot of manual effort and is not easily automated. If you are in a position where you can influence data collection, we highly recommend avoiding manually entered values for concepts that are better captured using categorical variables.\n",
    "\n",
    "\n",
    "Often, manually entered values do not correspond to fixed categories, but still have some underlying structure, like addresses, names of places or people, dates, telephone numbers, or other identifiers. These kinds of strings are often very hard to parse, and their treatment is highly dependent on context and domain. A systematic treatment of these cases is beyond the scope of this book.\n",
    "\n",
    "\n",
    "The final category of string data is freeform text data that consists of phrases or sentences. Examples include tweets, chat logs, and hotel reviews, as well as the collected works of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection of 50,000 ebooks. All of these collections contain information mostly as sentences composed of words.1 For simplicity’s sake, let’s assume all our documents are in one language, English.2 In the context of text analysis, the dataset is often called the corpus, and each data point, represented as a single text, is called a document. These terms come from the information retrieval (IR) and natural language processing (NLP) community, which both deal mostly in text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f8b0c-7196-415f-bf9c-68eca21eaeb6",
   "metadata": {},
   "source": [
    "# Example application: sentiment analysis of movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c83cf75-acd3-4a2f-b515-72f550d89c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = r\"./Raw Data/aclImdb/\"\n",
    "reviews_train = sklearn.datasets.load_files(path_data+\"train\", categories=[\"pos\", \"neg\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "830c25e1-0c49-46aa-a57f-77192d1da0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "\n",
      "text_train[1]:\n",
      "b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'\n",
      "\n",
      "y_train[1]:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target;\n",
    "\n",
    "print(\"type of text_train: {}\".format(type(text_train)));\n",
    "print(\"length of text_train: {}\".format(len(text_train)));\n",
    "print(\"\\ntext_train[1]:\\n{}\".format(text_train[1]));\n",
    "print(\"\\ny_train[1]:\\n{}\".format(y_train[1]));\n",
    "\n",
    "# remove unicode back spaces\n",
    "text_train = [doc.replace(b\"<br />\",b\" \") for doc in text_train];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "3821a6b7-c0d6-4f8d-8acd-7dbf9eecdad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test = sklearn.datasets.load_files(path_data+\"test\", categories=[\"pos\", \"neg\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "fe4a1947-c30d-428d-bf9a-25f157428b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "text_test, y_test = reviews_test.data, reviews_test.target;\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test];\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)));\n",
    "print(\"Samples per class (test): {}\".format(numpy.bincount(y_test)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b14e8-ac8e-432f-8896-d3ad0995f384",
   "metadata": {},
   "source": [
    "## Representing Text Data as a Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1c67ca-c4cb-4314-ba9e-21abea2d6c0b",
   "metadata": {},
   "source": [
    "One of the most simple but effective and commonly used ways to represent text for machine learning is using the bag-of-words representation. When using this represen‐ tation, we discard most of the structure of the input text, like chapters, paragraphs, sentences, and formatting, and only count how often each word appears in each text in the corpus. Discarding the structure and counting only word occurrences leads to the mental image of representing text as a “bag.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e999d1-3154-4ca5-9b67-234d199c9c2b",
   "metadata": {},
   "source": [
    "Computing the bag-of-words representation for a corpus of documents consists of the following three steps\n",
    "\n",
    "1) $Tokenization$. Split each document into the words that appear in it (called tokens),\n",
    "for example by splitting them on whitespace and punctuation.\n",
    "2) $Vocabulary$ building. Collect a vocabulary of all words that appear in any of the\n",
    "documents, and number them (say, in alphabetical order)\n",
    "3) $Encoding$. For each document, count how often each of the words in the vocabulary appear in this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ee97e35a-d691-475b-beea-17f6e097bb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = sklearn.feature_extraction.text.CountVectorizer();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "4b2d27f9-e3d8-474f-b808-1e01e6c813c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bards_words =[\"The fool doth think he is wise,\",\n",
    "\"but the wise man knows himself to be a fool\"];\n",
    "vect.fit(bards_words);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "d8813ef7-103d-49e5-bbfb-9731db70199f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "Vocabulary content:\n",
      " {'the': 9, 'fool': 3, 'doth': 2, 'think': 10, 'he': 4, 'is': 6, 'wise': 12, 'but': 1, 'man': 8, 'knows': 7, 'himself': 5, 'to': 11, 'be': 0}\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)));\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "9598f12e-ec91-425d-b1cc-962a82c04be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 16 stored elements in Compressed Sparse Row format>\n",
      "\n",
      "  (0, 2)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 9)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "bag_of_words = vect.transform(bards_words);\n",
    "print(\"bag_of_words: {}\\n\".format(repr(bag_of_words)));\n",
    "print(bag_of_words);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5181ee5f-79a5-4924-83ab-20b070fe33c7",
   "metadata": {},
   "source": [
    "The bag-of-words representation is stored in a SciPy sparse matrix that only stores the entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one row for each of the two data points and one feature for each of the words in the vocabulary. A sparse matrix is used as most documents only contain a small subset of the words in the vocabulary, meaning most entries in the feature array are 0. Think about how many different words might appear in a movie review compared to all the words in the English language (which is what the vocabulary models). Storing all those zeros would be prohibitive, and a waste of memory. To look at the actual con‐ tent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores all the 0 entries) using the toarray method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "f913d899-4b4e-4277-a295-7715a6f1b162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense representation of bag_of_words:\n",
      "[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n",
      " [1 1 0 1 0 1 0 1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Dense representation of bag_of_words:\\n{}\".format(bag_of_words.toarray()));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee240bf-c90c-49d0-bbd3-a01c320665f1",
   "metadata": {},
   "source": [
    "We can see that the word counts for each word are either 0 or 1; neither of the two strings in bards_words contains a word twice. Let’s take a look at how to read these feature vectors. The first string (\"The fool doth think he is wise,\") is repre‐ sented as the first row in, and it contains the first word in the vocabulary, \"be\", zero times. It also contains the second word in the vocabulary, \"but\", zero times. It con‐ tains the third word, \"doth\", once, and so on. Looking at both rows, we can see that the fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\", appear in both strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db62acf-50bd-4414-bf2e-86797d3bed53",
   "metadata": {},
   "source": [
    "## Bag-of-words for movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "1f309301-a53e-4cdc-859f-436d22c27705",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = sklearn.feature_extraction.text.CountVectorizer();\n",
    "X_train = vect.fit_transform(text_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "45a22eb4-1975-485b-8535-dcb6236070c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:\n",
      "<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3431196 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train:\\n{}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "ac54ffd3-abee-46a1-8e2d-b1659d13d656",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vect.get_feature_names_out();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "0ebb5f03-389c-471e-99d9-5feea6e50607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 74849\n",
      "\n",
      "First 20 features:\n",
      "['00' '000' '0000000000001' '00001' '00015' '000s' '001' '003830' '006'\n",
      " '007' '0079' '0080' '0083' '0093638' '00am' '00pm' '00s' '01' '01pm' '02']\n",
      "\n",
      "Features 20010 to 20030:\n",
      "['dratted' 'draub' 'draught' 'draughts' 'draughtswoman' 'draw' 'drawback'\n",
      " 'drawbacks' 'drawer' 'drawers' 'drawing' 'drawings' 'drawl' 'drawled'\n",
      " 'drawling' 'drawn' 'draws' 'draza' 'dre' 'drea']\n",
      "\n",
      "Every 2000th feature:\n",
      "['00' 'aesir' 'aquarian' 'barking' 'blustering' 'bête' 'chicanery'\n",
      " 'condensing' 'cunning' 'detox' 'draper' 'enshrined' 'favorit' 'freezer'\n",
      " 'goldman' 'hasan' 'huitieme' 'intelligible' 'kantrowitz' 'lawful' 'maars'\n",
      " 'megalunged' 'mostey' 'norrland' 'padilla' 'pincher' 'promisingly'\n",
      " 'receptionist' 'rivals' 'schnaas' 'shunning' 'sparse' 'subset'\n",
      " 'temptations' 'treatises' 'unproven' 'walkman' 'xylophonist']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of features: {}\\n\".format(len(feature_names)))\n",
    "print(\"First 20 features:\\n{}\\n\".format(feature_names[:20]))\n",
    "print(\"Features 20010 to 20030:\\n{}\\n\".format(feature_names[20010:20030]))\n",
    "print(\"Every 2000th feature:\\n{}\\n\".format(feature_names[::2000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7be7d8-d01f-4e8f-9cc2-d9c7e12e1301",
   "metadata": {},
   "source": [
    "As you can see, possibly a bit surprisingly, the first 10 entries in the vocabulary are all\n",
    "numbers. All these numbers appear somewhere in the reviews, and are therefore\n",
    "extracted as words. Most of these numbers don’t have any immediate semantic mean‐\n",
    "ing—apart from \"007\", which in the particular context of movies is likely to refer to\n",
    "the James Bond character.5 Weeding out the meaningful from the nonmeaningful\n",
    "“words” is sometimes tricky. Looking further along in the vocabulary, we find a col‐\n",
    "lection of English words starting with “dra”. You might notice that for \"draught\",\n",
    "\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\n",
    "vocabulary as distinct words. These words have very closely related semantic mean‐\n",
    "ings, and counting them as different words, corresponding to different features,\n",
    "might not be ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d337ec8a-3e0c-47b7-9eaf-1804bd0edc8b",
   "metadata": {},
   "source": [
    "Challenge: Numbers appear in the first 20 words of the given data. The word \"007\" likely refers to James Bond. Verify this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "9296c1f8-af2c-4de9-aa87-0516d7abb789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps\n",
    "# Filter list where 007 apears, this can be done with a \"map\" key word byt it mighty be messy in a single line\n",
    "idx = [];\n",
    "for i, x in enumerate(X_train):\n",
    "    word_vals = x.toarray()[0];\n",
    "    if word_vals[vect.vocabulary_[\"007\"]] > 0: idx.append(i);        # filter out 007 word\n",
    "\n",
    "    \n",
    "# apply idx reviews list \n",
    "arrTxt = numpy.array(text_train);    # convert text_train list to array to used indecises\n",
    "arrBondJamesBond = arrTxt[idx];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "b3bc1840-77ce-4cd6-8645-7fd6c04a24dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'EA have shown us that they can make a classic 007 agent and make you feel in the 60\\'s world. The graphics of the game are outstanding and also the voice recording is very professional. I got this game April 2007 (two years after release), and I am still impressed with the gameplay. It\\'s a shame that EA will no longer make 007 games.  I give this game 10/10 for the levels it contains, especially the \"consulate\" level. I would recommend this game to anyone from the age of 13 and over. The only thing I didn\\'t like in the game is the Russian boat level, it was too much pressure. On the whole I like the game A LOT!!'\n"
     ]
    }
   ],
   "source": [
    "# pritn random review\n",
    "print(arrBondJamesBond[numpy.random.randint(0,len(idx))]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9b0107-830e-4421-a4dd-acbbcebbcae5",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8a8f2-38e2-4b22-b9da-a1c4dd4bc61d",
   "metadata": {},
   "source": [
    "Before we try to improve our feature extraction, let’s obtain a quantitative measure of performance by actually building a classifier. We have the training labels stored in y_train and the bag-of-words representation of the training data in X_train, so we can train a classifier on this data. For high-dimensional, sparse data like this, linear models like LogisticRegression often work best. Let’s start by evaluating LogisticRegresssion using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "e0a93fc5-fc14-4cf9-8497-1780af930feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = sklearn.linear_model.LogisticRegression(max_iter=1000);\n",
    "scores = sklearn.model_selection.cross_val_score(logreg, X_train, y_train, cv=5,n_jobs = 3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "548d0399-ad3c-462e-b174-eb27ac1160c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 88.13 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean cross-validation accuracy: {:4.2f} %\".format(100*scores.mean()));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a821e6a-af7b-4306-84fc-6a5e2913b20a",
   "metadata": {},
   "source": [
    "We obtain a mean cross-validation score of 88%, which indicates reasonable perfor‐ mance for a balanced binary classification task. We know that LogisticRegression has a regularization parameter, C, which we can tune via cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "02df871f-c0b5-45f0-88b1-30ae38f2121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C' : numpy.logspace(-3,1,5)};\n",
    "grid = sklearn.model_selection.GridSearchCV(logreg, param_grid=param_grid, n_jobs=3);\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "id": "1e859cfc-3897-4ba7-b11c-03936ef94407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 88.80 %\n",
      "Best parameters:  {'C': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:4.2f} %\".format(100*grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)\n",
    "best_para = grid.best_params_;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "7a6a7814-f3f5-41d8-a135-e10d17a0b49a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 87.89 %\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"Test score: {:4.2f} %\".format(100*grid.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3c5462-38c5-4ac8-ac4b-f0da23773c50",
   "metadata": {},
   "source": [
    "Now, let’s see if we can improve the extraction of words. The CountVectorizer\n",
    "extracts tokens using a regular expression. By default, the regular expression that is\n",
    "used is \"\\b\\w\\w+\\b\". If you are not familiar with regular expressions, this means it\n",
    "finds all sequences of characters that consist of at least two letters or numbers (\\w)\n",
    "and that are separated by word boundaries (\\b). It does not find single-letter words,\n",
    "and it splits up contractions like “doesn’t” or “bit.ly”, but it matches “h8ter” as a single\n",
    "word. The CountVectorizer then converts all words to lowercase characters, so that\n",
    "“soon”, “Soon”, and “sOon” all correspond to the same token (and therefore feature).\n",
    "This simple mechanism works quite well in practice, but as we saw earlier, we get\n",
    "many uninformative features (like the numbers). One way to cut back on these is to\n",
    "only use tokens that appear in at least two documents (or at least five documents, and\n",
    "so on). A token that appears only in a single document is unlikely to appear in the test\n",
    "set and is therefore not helpful. We can set the minimum number of documents a\n",
    "token needs to appear in with the min_df parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 674,
   "id": "c483b958-f628-4226-bcee-2055ecbef457",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect_2 = sklearn.feature_extraction.text.CountVectorizer(min_df=5);\n",
    "X_train = vect_2.fit_transform(text_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "id": "a533dca2-b962-473f-97df-1fcd6d73bbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with min_df: <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 3354014 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train with min_df: {}\".format(repr(X_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b9eb2-bd39-4c73-a7c7-79db49878052",
   "metadata": {},
   "source": [
    "By requiring at least five appearances of each token, we can bring down the number of features to 27,271, as seen in the preceding output—only about a third of the origi‐ nal features. Let’s look at some tokens again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "id": "19565ad7-aa78-496c-8c4a-4c6e13abe8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 50 features:\n",
      "['00' '000' '007' '00s' '01' '02' '03' '04' '05' '06' '07' '08' '09' '10'\n",
      " '100' '1000' '100th' '101' '102' '103' '104' '105' '107' '108' '10s'\n",
      " '10th' '11' '110' '112' '116' '117' '11th' '12' '120' '12th' '13' '135'\n",
      " '13th' '14' '140' '14th' '15' '150' '15th' '16' '160' '1600' '16mm' '16s'\n",
      " '16th']\n",
      "\n",
      "Features 20010 to 20030:\n",
      "['repentance' 'repercussions' 'repertoire' 'repetition' 'repetitions'\n",
      " 'repetitious' 'repetitive' 'rephrase' 'replace' 'replaced' 'replacement'\n",
      " 'replaces' 'replacing' 'replay' 'replayable' 'replayed' 'replaying'\n",
      " 'replays' 'replete' 'replica']\n",
      "\n",
      "Every 1000th feature:\n",
      "['00' 'alternatively' 'baked' 'bothersome' 'centipede' 'complicity'\n",
      " 'cutlery' 'disgraceful' 'elton' 'fatal' 'gaining' 'hamburgers' 'ideals'\n",
      " 'ivory' 'leering' 'martin' 'moxy' 'opportunist' 'picasso' 'prudish'\n",
      " 'repartee' 'sas' 'silvers' 'standup' 'talkative' 'trend' 'verisimilitude'\n",
      " 'wreaking']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_names = vect_2.get_feature_names_out();\n",
    "\n",
    "print(\"First 50 features:\\n{}\\n\".format(feature_names[0:50]));\n",
    "print(\"Features 20010 to 20030:\\n{}\\n\".format(feature_names[20010:20030]));\n",
    "print(\"Every 1000th feature:\\n{}\\n\".format(feature_names[::1000]));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cebe165-7669-48db-b37f-3c83be74a375",
   "metadata": {},
   "source": [
    "There are clearly many fewer numbers, and some of the more obscure words or mis‐ spellings seem to have vanished. Let’s see how well our model performs by doing a grid search again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 680,
   "id": "2c04137e-bb59-469d-b7b7-943be8808e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C' : numpy.logspace(-3,1,5)};\n",
    "logreg = sklearn.linear_model.LogisticRegression(max_iter=10000);\n",
    "grid = sklearn.model_selection.GridSearchCV(logreg, param_grid=param_grid, cv=5, n_jobs=4);\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "id": "498b6fda-7bb6-442f-8378-00e9b98a1f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 88.81 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:4.2f} %\".format(100*grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dced08-e4c8-4bab-82fd-192d66913158",
   "metadata": {},
   "source": [
    "The best validation accuracy of the grid search is still 89%, unchanged from before.\n",
    "We didn’t improve our model, but having fewer features to deal with speeds up pro‐\n",
    "cessing and throwing away useless features might make the model more interpretable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063612d-f959-4c2f-a8f2-ae3301453a75",
   "metadata": {},
   "source": [
    "If the transform method of CountVectorizer is called on a docu‐ ment that contains words that were not contained in the training data, these words will be ignored as they are not part of the dictio‐ nary. This is not really an issue for classification, as it’s not possible to learn anything about words that are not in the training data. For some applications, like spam detection, it might be helpful to add a feature that encodes how many so-called “out of vocabulary” words there are in a particular document, though. For this to work, you need to set min_df; otherwise, this feature will never be active dur‐\n",
    "ing training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312fcec9-a7dd-4947-89e9-e6690ba41263",
   "metadata": {},
   "source": [
    "## Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739af4da-60c2-4996-8906-1436cd7183ec",
   "metadata": {},
   "source": [
    "Another way that we can get rid of uninformative words is by discarding words that are too frequent to be informative. There are two main approaches: using a language- specific list of stopwords, or discarding words that appear too frequently. scikit- learn has a built-in list of English stopwords in the feature_extraction.text module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 694,
   "id": "4f811bc0-6f4f-4751-be26-13f550c2f473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 318\n",
      "Every 10th stopword:\n",
      "['cant', 'whence', 'noone', 'up', 'or', 'above', 'per', 'whoever', 'move', 'i', 'else', 'somewhere', 'forty', 'cannot', 'ever', 'put', 'last', 'anywhere', 'get', 'whatever', 'became', 'five', 'within', 'eight', 'very', 'have', 'hers', 'yourself', 'you', 'this', 'bottom', 'nobody']\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of stop words: {}\".format(len(sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)))\n",
    "print(\"Every 10th stopword:\\n{}\".format(list(sklearn.feature_extraction.text.ENGLISH_STOP_WORDS)[::10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 778,
   "id": "63171895-7e18-45da-8c1c-8f0b19467a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = sklearn.feature_extraction.text.CountVectorizer(stop_words=\"english\", min_df=5).fit(text_train);\n",
    "X_train = vect.transform(text_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 779,
   "id": "c86daba9-46eb-4e40-a758-946c5b1f7b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train with stop words:\n",
      "<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 2149958 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train with stop words:\\n{}\".format(repr(X_train)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a69c58-9d1b-40d3-8b6f-0ce8e26c4b05",
   "metadata": {},
   "source": [
    "There are now 305 (27,271–26,966) fewer features in the dataset, which means that\n",
    "most, but not all, of the stopwords appeared. Let’s run the grid search again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "555393d1-2734-4298-a404-e2bb06aaf9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {'C' : numpy.logspace(-3,1,5)};\n",
    "logreg = sklearn.linear_model.LogisticRegression(max_iter=10000);\n",
    "grid = sklearn.model_selection.GridSearchCV(logreg, param_grid=param_grid, cv=5, n_jobs=4);\n",
    "grid.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "22294f18-f7d7-43e1-8aec-fd16b73d06e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 88.30 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Best cross-validation score: {:4.2f} %\".format(100*grid.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57af855-7815-469b-ac9c-aaa3607c71d1",
   "metadata": {},
   "source": [
    "The grid search performance decreased slightly using the stopwords—not enough to worry about, but given that excluding 305 features out of over 27,000 is unlikely to change performance or interpretability a lot, it doesn’t seem worth using this list. Fixed lists are mostly helpful for small datasets, which might not contain enough information for the model to determine which words are stopwords from the data itself. As an exercise, you can try out the other approach, discarding frequently appearing words, by setting the max_df option of CountVectorizer and see how it\n",
    "influences the number of features and the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca213487-e37e-4a68-b6dd-00c29d24f069",
   "metadata": {},
   "source": [
    "## Rescaling the data with tf–idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "id": "33635e9e-ac18-4a0f-9fe0-660939d2f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#335"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9af5d6-4ec7-49db-bb04-2512209f4cef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
