{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b95524e-9ff7-49b3-951d-2eafb4a602bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import cm\n",
    "import pandas\n",
    "import mglearn\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import sklearn.ensemble              # import seperatley otherwise sub module won't be imported\n",
    "import sklearn.neural_network        # import seperatley otherwise sub module won't be imported\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.feature_selection\n",
    "\n",
    "import graphviz\n",
    "import mpl_toolkits.mplot3d as plt3dd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616e322f-029a-4f4a-a74e-2a8b0a8288ee",
   "metadata": {},
   "source": [
    "# Data represented as strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c468793-1304-49c3-a805-a8755df60804",
   "metadata": {},
   "source": [
    "There are four kinds of string data you might see:\n",
    "\n",
    "- Categorical data\n",
    "- Free strings that can be semantically mapped to categories\n",
    "- Structured string data\n",
    "- Text data\n",
    "\n",
    "Categorical data is data that comes from a fixed list. Say you collect data via a survey where you ask people their favorite color, with a drop-down menu that allows them to select from “red,” “green,” “blue,” “yellow,” “black,” “white,” “purple,” and “pink.” This will result in a dataset with exactly eight different possible values, which clearly encode a categorical variable. You can check whether this is the case for your data by eyeballing it (if you see very many different strings it is unlikely that this is a categorical variable) and confirm it by computing the unique values over the dataset, and possibly a histogram over how often each appears. You also might want to check whether each variable actually corresponds to a category that makes sense for your application. Maybe halfway through the existence of your survey, someone found that “black” was misspelled as “blak” and subsequently fixed the survey. As a result, your dataset contains both “blak” and “black,” which correspond to the same semantic meaning and should be consolidated\n",
    "\n",
    "\n",
    "Now imagine instead of providing a drop-down menu, you provide a text field for the users to provide their own favorite colors. Many people might respond with a color name like “black” or “blue.” Others might make typographical errors, use different spellings like “gray” and “grey,” or use more evocative and specific names like “midnight blue.” You will also have some very strange entries. Some good examples come from the xkcd Color Survey, where people had to name colors and came up with names like “velociraptor cloaka” and “my dentist’s office orange. I still remember his dandruff slowly wafting into my gaping yaw,” which are hard to map to colors automatically (or at all). The responses you can obtain from a text field belong to the second category in the list, free strings that can be semantically mapped to categories. It will probably be best to encode this data as a categorical variable, where you can select the categories either by using the most common entries, or by defining categories that will capture responses in a way that makes sense for your application. You might then have some categories for standard colors, maybe a category “multicolored” for people that gave answers like “green and red stripes,” and an “other” category for things that cannot be encoded otherwise. This kind of preprocessing of strings can take a lot of manual effort and is not easily automated. If you are in a position where you can influence data collection, we highly recommend avoiding manually entered values for concepts that are better captured using categorical variables.\n",
    "\n",
    "\n",
    "Often, manually entered values do not correspond to fixed categories, but still have some underlying structure, like addresses, names of places or people, dates, telephone numbers, or other identifiers. These kinds of strings are often very hard to parse, and their treatment is highly dependent on context and domain. A systematic treatment of these cases is beyond the scope of this book.\n",
    "\n",
    "\n",
    "The final category of string data is freeform text data that consists of phrases or sentences. Examples include tweets, chat logs, and hotel reviews, as well as the collected works of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection of 50,000 ebooks. All of these collections contain information mostly as sentences composed of words.1 For simplicity’s sake, let’s assume all our documents are in one language, English.2 In the context of text analysis, the dataset is often called the corpus, and each data point, represented as a single text, is called a document. These terms come from the information retrieval (IR) and natural language processing (NLP) community, which both deal mostly in text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3f8b0c-7196-415f-bf9c-68eca21eaeb6",
   "metadata": {},
   "source": [
    "# Example application: sentiment analysis of movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5c83cf75-acd3-4a2f-b515-72f550d89c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_data = r\"./Raw Data/aclImdb/\"\n",
    "reviews_train = sklearn.datasets.load_files(path_data+\"train\", categories=[\"pos\", \"neg\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "830c25e1-0c49-46aa-a57f-77192d1da0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of text_train: <class 'list'>\n",
      "length of text_train: 25000\n",
      "\n",
      "text_train[1]:\n",
      "b'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing only. You have too see it for yourself to get at grip of how horrible a movie really can be. Not that I recommend you to do that. There are so many clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here that will just make you cry. To start with the technical first, there are a LOT of mistakes regarding the airplane. I won\\'t list them here, but just mention the coloring of the plane. They didn\\'t even manage to show an airliner in the colors of a fictional airline, but instead used a 747 painted in the original Boeing livery. Very bad. The plot is stupid and has been done many times before, only much, much better. There are so many ridiculous moments here that i lost count of it really early. Also, I was on the bad guys\\' side all the time in the movie, because the good guys were so stupid. \"Executive Decision\" should without a doubt be you\\'re choice over this one, even the \"Turbulence\"-movies are better. In fact, every other movie in the world is better than this one.'\n",
      "\n",
      "y_train[1]:\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = reviews_train.data, reviews_train.target;\n",
    "\n",
    "print(\"type of text_train: {}\".format(type(text_train)));\n",
    "print(\"length of text_train: {}\".format(len(text_train)));\n",
    "print(\"\\ntext_train[1]:\\n{}\".format(text_train[1]));\n",
    "print(\"\\ny_train[1]:\\n{}\".format(y_train[1]));\n",
    "\n",
    "# remove unicode back spaces\n",
    "text_train = [doc.replace(b\"<br />\",b\" \") for doc in text_train];\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3d158cd3-ec5e-468c-b198-2492c6e8dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples per class (training): [12500 12500]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3821a6b7-c0d6-4f8d-8acd-7dbf9eecdad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_test = sklearn.datasets.load_files(path_data+\"test\", categories=[\"pos\", \"neg\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fe4a1947-c30d-428d-bf9a-25f157428b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in test data: 25000\n",
      "Samples per class (test): [12500 12500]\n"
     ]
    }
   ],
   "source": [
    "text_test, y_test = reviews_test.data, reviews_test.target\n",
    "print(\"Number of documents in test data: {}\".format(len(text_test)))\n",
    "print(\"Samples per class (test): {}\".format(numpy.bincount(y_test)))\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
