{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "128f4a5f-cea8-4aa4-8c70-526159b8c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import cm\n",
    "import pandas\n",
    "import mglearn\n",
    "\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import sklearn.ensemble              # import seperatley otherwise sub module won't be imported\n",
    "import sklearn.neural_network        # import seperatley otherwise sub module won't be imported\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.feature_selection\n",
    "\n",
    "import graphviz\n",
    "import mpl_toolkits.mplot3d as plt3dd\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "import nltk     # language processing packages for lemmatization\n",
    "import spacy \n",
    "\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93ded11-f4f6-4813-bcaf-03eefd5139ee",
   "metadata": {},
   "source": [
    "# Data crunching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7060498a-b3fe-417d-aee3-61bf3b97a3b9",
   "metadata": {},
   "source": [
    "## Topic modeling and document clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6348f404-c3d8-4258-91d4-1ee7ad34b853",
   "metadata": {},
   "source": [
    "One particular technique that is often applied to text data is topic modeling, which is an umbrella term describing the task of assigning each document to one or multiple topics, usually without supervision. A good example for this is news data, which might be categorized into topics like “politics,” “sports,” “finance,” and so on. If each document is assigned a single topic, this is the task of clustering the documents, as discussed in Chapter 3. If each document can have more than one topic, the task relates to the decomposition methods from Chapter 3. Each of the components we learn then corresponds to one topic, and the coefficients of the components in the representation of a document tell us how strongly related that document is to a particular topic. Often, when people talk about topic modeling, they refer to one particular decomposition method called Latent Dirichlet Allocation (often LDA for short)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcd434-d65d-4014-84f3-a50e0ec7350f",
   "metadata": {},
   "source": [
    "### Latent dirichlet allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53be5ef4-53af-4206-8598-6680965448d2",
   "metadata": {},
   "source": [
    "Intuitively, the LDA model tries to find groups of words (the topics) that appear together frequently. LDA also requires that each document can be understood as a “mixture” of a subset of the topics. It is important to understand that for the machine learning model a “topic” might not be what we would normally call a topic in everyday speech, but that it resembles more the components extracted by PCA or NMF (which we discussed in Chapter 3), which might or might not have a semantic meaning. Even if there is a semantic meaning for an LDA “topic”, it might not be something we’d usually call a topic. Going back to the example of news articles, we might have a collection of articles about sports, politics, and finance, written by two specific authors. In a politics article, we might expect to see words like “governor,” “vote,” “party,” etc., while in a sports article we might expect words like “team,” “score,” and “season.” Words in each of these groups will likely appear together, while it’s less likely that, for example, “team” and “governor” will appear together. However, these are not the only groups of words we might expect to appear together. The two reporters might prefer different phrases or different choices of words. Maybe one of them likes to use the word “demarcate” and one likes the word “polarize.” Other “topics” would then be “words often used by reporter A” and “words often used by reporter B,” though these are not topics in the usual sense of the word. \n",
    "\n",
    "Let’s apply LDA to our movie review dataset to see how it works in practice. For unsupervised text document models, it is often good to remove very common words, as they might otherwise dominate the analysis. We’ll remove words that appear in at least 20 percent of the documents, and we’ll limit the bag-of-words model to the 10,000 words that are most common after removing the top 20 percent:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a7695-5593-4858-a2cd-eeaddca40007",
   "metadata": {},
   "source": [
    "Movie review data can be downloaded from $\\href{https://ai.stanford.edu/\\%7Eamaas/data/sentiment/}{link}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "248eaf9e-ac3c-4424-9401-9b02cb42ed13",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"./Raw Data/aclImdb/\";\n",
    "reviews_train = sklearn.datasets.load_files(path + \"train\", categories=[\"pos\",\"neg\"]);\n",
    "reviews_test = sklearn.datasets.load_files(path + \"test\", categories=[\"pos\",\"neg\"])\n",
    "\n",
    "text_train, y_train = reviews_train.data, reviews_train.target;\n",
    "text_test, y_test = reviews_train.data, reviews_train.target;\n",
    "\n",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test];\n",
    "text_train = [doc.replace(b\"<br />\",b\" \") for doc in text_train];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3462edc4-7d88-4c93-85c4-b733a957afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = sklearn.feature_extraction.text.CountVectorizer(max_features=10000, max_df=0.15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b952a552-f03d-453a-a741-5cc712f21c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "vect.fit(text_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5f68fb-feea-4388-8557-545bd4f933c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vect.transform(text_train);\n",
    "X_test = vect.transform(text_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f09e980-dd84-480c-970c-2a639775ddfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = sklearn.decomposition.LatentDirichletAllocation(n_components=10, n_jobs=4, learning_method=\"batch\", max_iter=25, random_state=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99d42e80-3fc3-4278-b8ef-ae6d54748d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We build the model and transform the data in one step\n",
    "# Computing transform takes some time,\n",
    "# and we can save time by doing both at once\n",
    "\n",
    "document_topics = lda.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5353dd63-a822-4392-a688-8448a90476c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of docs with primary topic #:\n",
      "\n",
      "Topic #1 : 4343\n",
      "\n",
      "Topic #2 : 2209\n",
      "\n",
      "Topic #3 : 1934\n",
      "\n",
      "Topic #4 : 1329\n",
      "\n",
      "Topic #5 : 5120\n",
      "\n",
      "Topic #6 : 4064\n",
      "\n",
      "Topic #7 : 1346\n",
      "\n",
      "Topic #8 : 1654\n",
      "\n",
      "Topic #9 : 830\n",
      "\n",
      "Topic #10 : 2171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a_max = document_topics.argmax(axis=1);\n",
    "bc = numpy.bincount(a_max)\n",
    "print(\"Number of docs with primary topic #:\\n\")\n",
    "for i, _count in enumerate(bc.ravel()):\n",
    "    print(f\"Topic #{i+1} : {_count}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ca96ed-0e9f-49ce-b857-f92ed9d3597c",
   "metadata": {},
   "source": [
    "Like the decomposition methods we saw in Chapter 3, LatentDirichletAllocation\n",
    "has a components_ attribute that stores how important each word is for each topic.\n",
    "The size of components_ is (n_topics, n_words):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "26412ae4-8a33-4f61-b180-7222e1c60621",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10000)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6bb74b4e-f30f-47da-a19d-348fe6fb966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic (a row in the components_), sort the features (ascending)\n",
    "# Invert rows with [:, ::-1] to make sorting descending\n",
    "sorting_mask = lda_components.argsort(axis=1)[:,::-1];\n",
    "feature_names = numpy.array(vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "62441d64-04af-4813-aad5-cdeb90caec3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "between       war           funny         show          didn          \n",
      "family        world         comedy        series        saw           \n",
      "young         us            guy           episode       thought       \n",
      "real          american      laugh         tv            am            \n",
      "us            our           jokes         episodes      thing         \n",
      "director      documentary   fun           shows         got           \n",
      "work          history       humor         season        10            \n",
      "both          years         re            new           want          \n",
      "beautiful     new           hilarious     years         going         \n",
      "each          human         doesn         television    watched       \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "action        kids          role          performance   horror        \n",
      "effects       action        cast          role          house         \n",
      "nothing       animation     john          john          killer        \n",
      "budget        children      version       actor         gets          \n",
      "script        game          novel         cast          woman         \n",
      "minutes       disney        director      plays         dead          \n",
      "original      fun           both          jack          girl          \n",
      "director      old           played        michael       around        \n",
      "least         10            mr            oscar         goes          \n",
      "doesn         kid           young         father        wife          \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n",
    "sorting=sorting_mask, topics_per_chunk=5, n_words=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
