{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09a5ccdb-5c48-4e78-a0cd-d6c068637046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import cm\n",
    "import pandas\n",
    "import mglearn\n",
    "import os\n",
    "import scipy\n",
    "\n",
    "import sklearn\n",
    "import sklearn.ensemble              # import seperatley otherwise sub module won't be imported\n",
    "import sklearn.neural_network        # import seperatley otherwise sub module won't be imported\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.feature_selection\n",
    "\n",
    "import graphviz\n",
    "import mpl_toolkits.mplot3d as plt3dd\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28af09d-fb61-4845-b3e8-0049c3f41e2f",
   "metadata": {},
   "source": [
    "When selecting a metric, you should always have the end goal of the machine learning application in mind. In practice, we are usually interested not just in making accurate predictions, but in using these predictions as part of a larger decisionmaking process. Before picking a machine learning metric, you should think about the high-level goal of the application, often called the $business$ $metric$.\n",
    "\n",
    "When choosing a model or adjusting parameters, you should pick the model or parameter values that have the most positive influence on the business metric. Often this is hard, as assessing the business impact of a particular model might require putting it in production in a real-life system.\n",
    "\n",
    "\n",
    "In the early stages of development, and for adjusting parameters, it is often infeasible to put models into production just for testing purposes, because of the high business or personal risks that can be involved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c62c67-0e36-4a53-9033-f045a2b5cce4",
   "metadata": {},
   "source": [
    "# Metrics for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e013916-1374-45df-a8b2-20b7ab750e43",
   "metadata": {},
   "source": [
    "Binary classification is arguably the most common and conceptually simple application of machine learning in practice. However, there are still a number of caveats in evaluating even this simple task.\n",
    "\n",
    "Remember that for binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for.\n",
    "\n",
    "Often, accuracy is not a good measure of predictive performance, as the number of mistakes we make does not contain all the information we are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69843b53-707a-4d81-90ba-be324d4c79d7",
   "metadata": {},
   "source": [
    "## Kinds of errors: Imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "013b0ec8-acd1-478d-bbb0-4c4ce1b9061d",
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = sklearn.datasets.load_digits();\n",
    "\n",
    "y = digits.target == 9;\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(digits.data, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ea3e70-fcc9-40ae-aa7b-07839241c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_majority = sklearn.dummy.DummyClassifier(strategy=\"most_frequent\").fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9a10519-588b-412d-9b4b-e03d7bf93f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique predicted labels: [False]\n",
      "Test score: 89.56 %\n"
     ]
    }
   ],
   "source": [
    "pred_most_frequent = dummy_majority.predict(X_test);\n",
    "\n",
    "print(\"Unique predicted labels: {}\".format(numpy.unique(pred_most_frequent)));\n",
    "print(\"Test score: {:4.2f} %\".format(100*dummy_majority.score(X_test, y_test)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62400e18-08a9-4065-8217-2477005180ec",
   "metadata": {},
   "source": [
    "We obtained close to 90% accuracy without learning anything. This might seem striking, but think about it for a minute. Imagine someone telling you their model is 90% accurate. You might think they did a very good job. But depending on the problem, that might be possible by just predicting one class! Let’s compare this against using an actual classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c5f6eeb-9315-4821-a793-d185ccdb1ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 91.78 %\n"
     ]
    }
   ],
   "source": [
    "tree = sklearn.tree.DecisionTreeClassifier(max_depth=2).fit(X_train, y_train);\n",
    "pred_tree = tree.predict(X_test); \n",
    "print(\"Test score: {:4.2f} %\".format(100*tree.score(X_test, y_test)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f799157-8317-4472-9edc-e4f90f2f9c99",
   "metadata": {},
   "source": [
    "According to accuracy, the DecisionTreeClassifier is only slightly better than the constant predictor. This could indicate either that something is wrong with how we used DecisionTreeClassifier, or that accuracy is in fact not a good measure here.\n",
    "\n",
    "For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier, which makes random predictions but produces classes with the same proportions as in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a50d5de4-04d5-4185-ae05-f367fc901381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy score: 89.56 %\n",
      "logreg score: 98.44 %\n"
     ]
    }
   ],
   "source": [
    "dummy = sklearn.dummy.DummyClassifier().fit(X_train, y_train);\n",
    "pred_dummy = dummy.predict(X_test);\n",
    "print(\"dummy score: {:4.2f} %\".format(100*dummy.score(X_test, y_test)));\n",
    "\n",
    "\n",
    "logreg = sklearn.linear_model.LogisticRegression(C=0.1, max_iter=1000).fit(X_train, y_train);\n",
    "pred_logreg = logreg.predict(X_test);\n",
    "print(\"logreg score: {:4.2f} %\".format(100*logreg.score(X_test, y_test)));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31b716-677c-4402-a058-bf98bb211257",
   "metadata": {},
   "source": [
    "## Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365c9651-66c7-4f86-bae9-4a76659e452d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
